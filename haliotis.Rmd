---
title: "Classifieur baysien naïf avec *Haliotis Rubra*"
author: "Jonathan Nguyen"
date: "14/10/2019"
output: html_document
---

# Introduction
*Haliotis rubra* est un mollusque endémique à l'Australie. L'objectif de ce projet est d'utiliser un classifieur bayesien naïf pour classifer une observation 'problem' selon son sexe: 'M' ou 'F'. Cette approche est largement basé sur l'exemple présenté avec iris dans le document de cours.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
```

# Le jeu de données

Le jeu de données utilisé provient du UC Irvine Machien Learning Repository et le propriétaire initial est le Department of Primary Industry and Fisheries, Tasmania. Ce jeu comporte de 4177 observations et 9 variables.

La fonction filter a été utilisé pour exclure 'I' pour infant de la variable Sex pour pouvoir travailleur avec une classifcation binaire.

```{r read csv}
haliotis_read <- read_csv('haliotis.csv')
haliotis <- filter(haliotis_read, Sex != "I")
haliotis

```


# Training et test
Le découpage a été utilsé pour séparé le jeu de données en 2. Une observation qui servira de test pour le classifieur et le restant du jeu pour construire le classifieur.

```{r train et test}
problem <- haliotis[1,] #la première rangée seulement
training <- haliotis[-1,] #tous sauf la première rangée
problem

```


# Sommaire

Les fonctions gather, group_by et summarize ont été utilisées pour déterminer la moyenne et l'écart type de 3 variables pour 'F' et 'M'. Ceci sera utile lorsqu'on travaillera avec la distribution normale.
```{r summary}
long_training <- training %>% 
  gather(variable, value, Length:Height, -Diameter, Whole_weight) #gather les valeurs des colonnes length, height et whole_weight, mais pas diameter

summary_statistics <- long_training %>% 
  group_by(Sex, variable) %>% 
  summarize(mean = mean(value), sd = sd(value))

summary_statistics
```


# Un test préliminaire

On filter une variable 'Meight' chez 'M' et sa moyenne et son écart-type. C'est 2 derniers servent d'argument pour dnorm qui donne la densité de probabilité, le likelihood.
```{r}
male_height <- filter(summary_statistics, Sex == 'M', variable == 'Height')
male_height
```

```{r dnorm}
proba_height_male <- dnorm(
  problem$'Height',
  male_height$mean,
  male_height$sd
)

proba_height_male #likelihood (densité de probabilité) d'avoir problem$'Height' sachant M
```

# Construction d'une fonction
Une fonction est codée prenant comme arguments le sex, la variable, le tableau sommaire, et l'observation test.
```{r function}
proba_class_knowing_feature <- function(class_name, feature_name, summary, problem){
  class_feature <- subset(summary,(Sex == class_name) & (variable == feature_name)) #cherche la bonne rangée dans summary_statistics
  
  proba_feature_class <- dnorm(
    as.double(problem[1, feature_name]), #as.double est nécessaire sinon retourne valeur non-numérique; donne valeur dans la colonne de feature_anme
    class_feature$mean,
    class_feature$sd)
  return(proba_feature_class)
}

proba_class_knowing_feature("M", "Height", summary_statistics, problem)
```


# Généralisation de la fonction avec map2_dbl
Ceci permet d'avoir le likehood pour toutes les variables de 'problem'.

```{r}
proba_class_feature <- summary_statistics %>% 
  mutate(id = map2_dbl(Sex, variable, proba_class_knowing_feature, summary = summary_statistics, problem = problem))

proba_class_feature
```


```{r}
table(training$Sex)
```


# Calculs

F: 
- prévalence: 0.46
2.88 x 1.64 x 0.43 x 0.46 = 0.93
<br> M:
- prévalence: 0.54
3.08 x 2.27 x 0.51 x 0.46 = 1.93

Notre classifieur mettra donc 'problem' dans la catégorie 'M' qui est la bonne réponse.

```{r}
problem
```


# Bibliotgraphie
https://archive.ics.uci.edu/ml/datasets/Abalone
http://www.r-tutor.com/r-introduction/data-frame/data-frame-row-slice
https://bookdown.org/ndphillips/YaRrr/slicing-dataframes.html
https://www.statmethods.net/management/subset.html
https://garrettgman.github.io/tidying/